# PbyP_CQ

Dados financeiros CQ

## Table of Contents

- [1 Tools](#1-tools-and-technologies-used)
- [2 Main Objectives](#2-main-objectives)
  - [2.1 Some Disclaimers](#21-some-disclaimers)
    - [2.1.1 Business Disclaimers](#212-business-disclaimers)
    - [2.1.2 Technical Disclaimers](#212-technical-disclaimers)
- [3 Output Locations](#3-outputs-and-outputs-location)
- [4 How it works](#4-how-to-use-it)
  - [4.1 Flow process](#41-process-flow)
  - [4.1 Final user - how to use](#x-final-user---how-to-use-it)
- [Development Aspects]()

## 1 Tools and technologies used

![Python](https://img.shields.io/badge/Python-306998?style=for-the-badge&logo=python)
![Databricks](https://img.shields.io/badge/Databricks-FFFFFF?style=for-the-badge&logo=databricks)
![PySpark](https://img.shields.io/badge/PySpark-F4F4F4?style=for-the-badge&logo=apache-spark)
![SQL](https://img.shields.io/badge/SQL-F80000?style=for-the-badge&logo=sql)
![EDL](https://img.shields.io/badge/EDL-367C2B?style=for-the-badge)
![Power Automate](https://tinyurl.com/2s39t29b)
<!--all Links shortened with https://tinyurl.com/-->

This project relies mainly on Pandas 1.4.4, spark 3.3.0 and runs on a DBR 11.3 LTS Runtime.  
The choice for `pandas` over `pyspark` was based both on performance — as there's complexity on the data extraction, but not that much data volume except for Data Blocks extraction, the processes became slower when using pyspark's API — and future maintenance — pandas has been the standard for small datasets and documented all over the internet, but not only that, most of the operations (if not all of them) can be used with `pyspark.pandas` API with small changes to the code.

[Future improvements](#x-future-improvements) are discussed below.

## 2 Main objectives

1. Automate the extraction, computation and creation of pbyp processes.
2. Create easily accessible and centralize pbyp data, providing tables and ways to relating them.
3. Provide historical data for pbyp. The data can go back up to 2014.
4. This repo focuses on obtaining data for CQ (Horizontina Factory) and it's specific context (there's a section talking about some [business rules](#x-business-rules)). For it to scale, many changes should be made, but most of the code can be used (many thing can also be improved as well).

### 2.1 Some disclaimers

**Repeated disclaimers mean their apply to both business and technical aspects and should be achieved together and are denoted by an "\*"**

### 2.1.2 Business disclaimers

- Much of the development was hindered by the lack of permissions due to data sensibility and importance. This makes some processes to not be made in an ideal way. THIS is a main problem, but we can extract all datablocks this way, just not the ideal as there should be some dataset containing this infos (if not, why????).
- We can always create repeated repos just adding parameters for each factory and their needs (this takes less effort, though does not centralize data).

### 2.1.2 Technical disclaimers

- We can always create repeated repos just adding parameters for each factory and their needs (this takes less effort, though does not centralize data).
- Some of the Data is availalble just for the last fiscal month, but can be made available by DACO if needed.
- Some functions use eval method, which can be dangerous, though for this use case was the fastest option for now. Vectorized operations can be implemented in the future.
- The number at the beginning of the notebook name represents the steps of the flow, but this is not a good practice. For now, they can be like that, but please change in the future.
- No major gains were obtained by using python's `concurrent.future` module.
- I do not think extracting directly from SharePoint is the best way, as I suppose there must be a SQL database with this info that can be

## 3 Outputs and outputs location

The tables generated by using this repo are located at FinanceCQ Dataset:

1. Most of them are located at pbyp_opening folder.
2. Data Block tables are located inside datablock_tables folder.

## 4 How to use it

### 4.1 Process flow

### 4.2 Final user - how to use it

TODO

### 5 Power automate flow

First, we check and filter if a new file arrived.

### 6 Sharepoint API

By using microsoft graph api, it was possible to obtain the files copied in the power automate flow directly from the designed folder (bpmocq/Automation). The functions used are defined in the notebook sharepoint_utils.  
The choice for a notebook instead of a module was because it's easier to just run another notebook from databricks in comparison to importing modules. It's easier to understand what's happeninng also.

#### 6.1 References

The following links were used to create the sharepoint api, as well as some other informations directly from Microsoft Learn and other resources such as stack overflow.

[Acessing sharepoint using app registration | deere.sharepoint](https://tinyurl.com/2dp4ywn5)  
[Sharepoint integration | deere.sharepoint](https://tinyurl.com/5dru8psy)  
[Working with Odata](https://learn.microsoft.com/en-us/odata/concepts/url-components)

### Data extraction

TODO

## Notebook docs

### x MAPPING TABLE

The objective of this notebook is to create the table with pbyp mapping eith the necessary filters to obtain pbyp_tool and also update if needed.

It does not need to be run everytime, just when and IF some update needs to be done (a new tool metric was added for example).

the tool metric values here disconsider Enterprise info, every Enterprise metric is considered as Allocated. If this is mapped in the future, the values "/JSON_FILES/pbyp_tool_mapping.json" must be updated.

### DATABLOCK

Data Block is a financial document that comes from *SOMEWHERE* and is saved first as is with DATABLOCK_SILVER notebook, for then being transformed in something useful for CQ (John Deere Horizontina).

### PBYP_TOOL

Document that is created based on financecq.datablock_gold table. To obtain it, one must filter each of the desired pbyb metrics, that are mapped in ***mapped_metrics (change this name later)***

## X Business rules

- When extracting, for now, there was no way to determine what is and what is not enterprise. This values are now summed together with rows starting with ALLOCATED.
- Someone must update currency_file manually (can be a problem for automated update process when calling a databricks job, as I don't know when the file was really modified/saved. Though if it's in EDL, this can maybe be done)
- For past years, just october months will be considered, as it's not a necessity for now (user defined use case). If this ever changes, user must update currency_file with the respective months and datablock of past years for each month.

## X Future Improvements

### X.1 Business future improvements and needs

### X.2 Technical future improvements

For scaling of this project, some changes I see that can become problems in the future:

- A lot of "repeated" code was used. Databricks Notebooks are a pain to work with local modules, but there are workarounds.
- pbyp_mappings.json file should be calculated in another way. To generalize for other factories (if it will be on day), some problems that might be encountered in the way the process is done now:
  1. the filtering in pbyp_mappings for source vs allocated can be better described and instead of fixed, passed as a variable.
- Conditions can be calculated another way, creating a table with them and specifying for which tables they apply. One could calculate them directly by evaluating one by one. I preferred to separate then in a json file and evaluate them after. I do think explicit is better than implicit and this change could be made in the future.
- There's a LOT of space for improvement in the sharepoint api usage and sharepoint data extraction:
  1. Make a class that access those functions (easy implementation, more organized code, non-core)
  2. Generalization for other factories: right now the parameters are set for CQ. The client-id is also set for CQ and is user dependent. Again, most of the problems are being caused by unauthorization.
  3. Some functions can be further improved, adding recursion for example, to test for children existence. e.g. get_drive_childrens_info for when there's no folder inside.
  4. I have no doubt that there are smarter ways to unfold those functions with the right HTTP API calls. The documentation from microsoft is a bit confusing about how to do it.
  5. Remove the parameters inside
- Currency file is being updated manually, but there are only 4 values, so it makes sense.
- ***Update the files by appending the new ones, instead of overwriting everytime (THIS WOULD BE AN AWESOME UPGRADE)***
- The namings are terrible, but they represent the flow of information. Needs changing.
- **Rebase ALL git until the point the secret is created. If not, there will be client_secret in git history.**
- Some part of the data manipulations rely heavily on string patterns (not complex string patterns). This can maybe cause problems if
- Maybe upload all data to EDL beforehand, instead of consuming it from sharepoint. This can be implemented usign sharepoint utils and tweaking the get_excel_files() function and setting the download path to somewhere in FinanceCQ dataset, probably using dbutils.

### Contributing

#### Main Contributors

[Lucas Eduardo Galdino Silva](silvalucase@johndeere.com) - Development  
[Raiza Galiotto](galiottoraizan@johndeere.com) - Key user and main business support  
[Camila Fagundes](fagundescamila@johndeere.com) - Key user and business support  
[Pablo Silva](silvapabloc2@johndeere.com) - Development  

#### Further help

[Filipe Pacheco](pachecofilipe@johndeere.com)  
[Osvaldo Bini](biniosvaldob2@johndeere.com)  
[Leonardo Silva](silvaleonardoa@johndeere.com)

### References

[trigger conditigon](https://techcommunity.microsoft.com/t5/azure-integration-services-blog/what-you-need-to-know-about-trigger-conditions/ba-p/2320757#:~:text=%40not%20%28empty%20%28triggerBody%20%28%29%29%29%20and%20%40contains%20%28triggerBody%20%28%29%3F,property%20contains%20%27.txt%27%20extension%20inside%20the%20trigger%20body)

[Get changes based on date and time](https://tomriha.com/how-to-get-changes-based-on-date-and-time-in-power-automate/#:~:text=The%20%E2%80%98Get%20changes%20for%20an%20item%20or%20a,users%20edit%20the%20item%20in%20the%20display%20form.)

[filter files based on created date](https://tomriha.com/how-to-filter-sharepoint-items-based-on-created-date-in-power-automate/#:~:text=When%20you%20filter%20items%20by%20their%20created%20date,date%2C%20and%20filter%20all%20the%20items%20in%20between.)
